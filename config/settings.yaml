# pipeline_mode:
#   - "no_llm": 仅使用 Spacy, BERT, 规则。速度极快，无需显存。
#   - "local": 使用本地加载的 GlobalLLM (如 GLM-4-9B, Llama-3)。
#   - "api": 使用外部 API (如 DeepSeek, GPT-4o)。
pipeline_mode: "local" 

# ================= API Configuration (仅当 mode="api" 时生效) =================
api_config:
  base_url: "https://api.deepseek.com/v1"
  api_key: "sk-xxxxxx"
  model_name: "deepseek-chat"
  temperature: 0.0
  
# A阶段：预处理
preprocess:
  use_llm_expansion: true  # 如果 mode="no_llm"，此项会被强制视为 false


recall:
  min_span_score: 0.90      # Span 的最高候选分低于此值则丢弃该 Span
  min_candidate_score: 0.80 # 候选词分低于此值则不作为备选
  bm25_topk: 100
  final_topk: 20
  model_path: "/share/home/202230275320/workspace/medical_coding/model/bge" # 模型路径
  obo_path: "data/hp.obo"

normalization:
  llm_threshold: 0.08  # 分数差距小于此值触发 LLM
  enable_llm: true


postcoordination:
  enable_llm: true
  max_new_tokens: 1024